{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24547,"status":"ok","timestamp":1763121865504,"user":{"displayName":"Murat Habibullaev","userId":"07739432263909480432"},"user_tz":-180},"id":"ErilgkCKEd0i","outputId":"f72529d7-7344-4a17-ebd5-7bedb4fff88d","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python-headless) (2.0.2)\n","Requirement already satisfied: dlib in /usr/local/lib/python3.12/dist-packages (19.24.6)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement recognition (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for recognition\u001b[0m\u001b[31m\n","\u001b[0mRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"]}],"source":["!pip install torch torchvision\n","!pip install opencv-python-headless\n","!pip install dlib\n","!pip install recognition\n","!pip install tqdm\n","!pip install -q torch torchvision\n","!pip install -q opencv-python-headless dlib face_recognition\n","!pip install face_recognition\n","!pip install dlib\n","\n"]},{"cell_type":"code","source":["# Veri indirme\n","from google.colab import files\n","files.upload()\n","\n","# Google Drive'ı bağla\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Gerekli kütüphaneleri kurulumu\n","!pip install -q kaggle kagglehub\n","\n","# Kaggle API anahtarını tekrar doğru yere taşıma\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/json\n","\n","# Google Drive'da verileri koyacağımız klasörü oluşturalım (varsa hata vermez)\n","!mkdir -p /content/drive/MyDrive/Deepfake-Detection-Project/datasets\n","\n","# VERİYİ İNDİR VE DOĞRUDAN GOOGLE DRIVE'A YÜKLE\n","import kagglehub\n","\n","# Veri setini tekrar Colab'ın geçici hafızasına indirelim\n","print(\"KaggleHub ile veri seti indirme işlemi başladı\")\n","path = kagglehub.dataset_download(\"xdxd003/ff-c23\")\n","print(f\"Veri seti başarıyla şu geçici yola indirildi: {path}\")\n","\n","# ZIP dosyasını BU SEFER DOĞRUDAN Google Drive'daki hedef klasörümüze çıkartalım\n","print(\"ZIP dosyası DOĞRUDAN Google Drive'a çıkartılıyor...\")\n","\n","!unzip -q \"{path}\"/*.zip -d /content/drive/MyDrive/Deepfake-Detection-Project/datasets/\n","\n","print(\"Veri seti başarıyla Google Drive'a çıkartıldı!\")"],"metadata":{"id":"2aY5crE7xboP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VİZE İÇİN BASE MODEL VERİ İŞLEME (250 REAL + 250 FAKE)\n","\n","import os\n","import cv2\n","import face_recognition\n","from google.colab import drive\n","from tqdm.notebook import tqdm\n","import glob\n","import random\n","\n","# Drive'ı bağla ve yolları tanımla\n","drive.mount('/content/drive')\n","\n","DRIVE_BASE_PATH = \"/content/drive/MyDrive/Deepfake-Detection-Project\"\n","RAW_DATA_PATH = os.path.join(DRIVE_BASE_PATH, \"datasets\", \"FaceForensics++_C23\")\n","# Çıktı klasörü\n","OUTPUT_PATH = os.path.join(DRIVE_BASE_PATH, \"processed_data_500_videos\")\n","\n","NUM_FRAMES_PER_VIDEO = 20 # Her videodan 20 kare alalım\n","IMAGE_SIZE = 224\n","\n","# Çıktı klasörlerini oluşturma\n","os.makedirs(os.path.join(OUTPUT_PATH, \"real\"), exist_ok=True)\n","os.makedirs(os.path.join(OUTPUT_PATH, \"fake\"), exist_ok=True)\n","\n","print(f\"Ham video veriler: {RAW_DATA_PATH}\")\n","print(f\"İşlenmiş yüz resimleri buraya kaydedilecek: {OUTPUT_PATH}\")\n","\n","def process_videos_final(video_paths, output_dir_label):\n","\n","    output_path = os.path.join(OUTPUT_PATH, output_dir_label)\n","\n","    for video_path in tqdm(video_paths, desc=f\"Processing {output_dir_label} videos\"):\n","        video_name = os.path.basename(video_path).split('.')[0]\n","        try:\n","            cap = cv2.VideoCapture(video_path)\n","            if not cap.isOpened(): continue\n","            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","            if total_frames < 1: continue\n","\n","            frame_indices = [int(i) for i in (total_frames / NUM_FRAMES_PER_VIDEO * (j + 0.5) for j in range(NUM_FRAMES_PER_VIDEO))] if total_frames > NUM_FRAMES_PER_VIDEO else list(range(total_frames))\n","\n","            frame_count = 0\n","            saved_frames = 0\n","            while saved_frames < len(frame_indices):\n","                ret, frame = cap.read()\n","                if not ret: break\n","                if frame_count in frame_indices:\n","                    face_locations = face_recognition.face_locations(frame)\n","                    if len(face_locations) > 0:\n","                        top, right, bottom, left = face_locations[0]\n","                        face_image = frame[top:bottom, left:right]\n","                        if face_image.size > 0:\n","                            resized_face = cv2.resize(face_image, (IMAGE_SIZE, IMAGE_SIZE))\n","                            save_name = f\"{video_name}_frame_{frame_count}.jpg\"\n","                            cv2.imwrite(os.path.join(output_path, save_name), resized_face)\n","                            saved_frames += 1\n","                frame_count += 1\n","            cap.release()\n","        except Exception as e:\n","            print(f\"Bir hata oluştu: {video_name} - {e}\")\n","            continue\n","\n","# VERİ SEÇİMİ VE OPERASYON BAŞLANGICI\n","\n","# Adım 1: 250 Gerçek video seçimi\n","real_videos = sorted(glob.glob(os.path.join(RAW_DATA_PATH, \"original\", \"*.mp4\")))[:250]\n","\n","# Adım 2: Her bir fake klasöründen 50'şer video seçimi\n","fake_video_folders = [\"Deepfakes\", \"Face2Face\", \"FaceShifter\", \"FaceSwap\", \"NeuralTextures\"]\n","fake_videos = []\n","for folder in fake_video_folders:\n","    videos_in_folder = sorted(glob.glob(os.path.join(RAW_DATA_PATH, folder, \"*.mp4\")))\n","    fake_videos.extend(videos_in_folder[:50])\n","\n","print(f\"İşlemek için {len(real_videos)} gerçek video seçildi.\")\n","print(f\"İşlemek için {len(fake_videos)} sahte video seçildi (her bir 5 fake dosyasından 50'şer adet).\")\n","\n","# Operasyon\n","process_videos_final(real_videos, \"real\")\n","process_videos_final(fake_videos, \"fake\")\n","\n","print(\"\\n\\n BASE MODEL İÇİN VERİ İŞLEME TAMAMLANDI!\")\n","print(f\"Tüm yüz resimleri Google Drive'daki '{OUTPUT_PATH}' klasörüne kaydedildi.\")"],"metadata":{"id":"TUEJP9BD0nTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model eğitimi\n","\n","\n","# BÖLÜM 1: Kütüphaneler ve Ayarlar\n","import torch, torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import os, glob\n","from sklearn.model_selection import train_test_split\n","from tqdm.notebook import tqdm\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","DRIVE_BASE_PATH = \"/content/drive/MyDrive/Deepfake-Detection-Project\"\n","DATA_PATH = os.path.join(DRIVE_BASE_PATH, \"processed_data_500_videos\")\n","CHECKPOINT_PATH = os.path.join(DRIVE_BASE_PATH, \"checkpoints\") # Kayıt klasörü\n","os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n","\n","LEARNING_RATE = 0.001\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 15\n","IMAGE_SIZE = 224\n","\n","# BÖLÜM 2: Dataset Sınıfı (Aynı)\n","class DeepfakeDataset(Dataset):\n","    def __init__(self, image_paths, labels, transform=None):\n","        self.image_paths, self.labels, self.transform = image_paths, labels, transform\n","    def __len__(self):\n","        return len(self.image_paths)\n","    def __getitem__(self, idx):\n","        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n","        label = torch.tensor([self.labels[idx]], dtype=torch.float32)\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","# BÖLÜM 3: Sıfırdan Basit CNN Modelimiz (Aynı)\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1, self.pool1 = nn.Conv2d(3, 16, 3, padding=1), nn.MaxPool2d(2, 2)\n","        self.conv2, self.pool2 = nn.Conv2d(16, 32, 3, padding=1), nn.MaxPool2d(2, 2)\n","        self.conv3, self.pool3 = nn.Conv2d(32, 64, 3, padding=1), nn.MaxPool2d(2, 2)\n","        self.fc1, self.fc2 = nn.Linear(64 * 28 * 28, 512), nn.Linear(512, 1)\n","        self.dropout = nn.Dropout(0.5)\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x)))\n","        x = self.pool2(F.relu(self.conv2(x)))\n","        x = self.pool3(F.relu(self.conv3(x)))\n","        x = x.view(-1, 64 * 28 * 28)\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# BÖLÜM 4: Veri Hazırlama ve Yükleyiciler (Aynı)\n","all_images = glob.glob(os.path.join(DATA_PATH, \"real\", \"*.jpg\")) + glob.glob(os.path.join(DATA_PATH, \"fake\", \"*.jpg\"))\n","all_labels = [0] * len(glob.glob(os.path.join(DATA_PATH, \"real\", \"*.jpg\"))) + [1] * len(glob.glob(os.path.join(DATA_PATH, \"fake\", \"*.jpg\")))\n","train_paths, val_paths, train_labels, val_labels = train_test_split(all_images, all_labels, test_size=0.2, random_state=42, stratify=all_labels)\n","transform = transforms.Compose([\n","    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","train_dataset, val_dataset = DeepfakeDataset(train_paths, train_labels, transform=transform), DeepfakeDataset(val_paths, val_labels, transform=transform)\n","train_loader, val_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True), DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# BÖLÜM 5: Model, Kayıp Fonksiyonu ve Optimizatör (Aynı)\n","model = SimpleCNN().to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# BÖLÜM 6: Eğitim Döngüsü (GÜNCELLENDİ)\n","best_accuracy = 0.0\n","\n","for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    corrects = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            preds = torch.sigmoid(outputs) > 0.5\n","            corrects += torch.sum(preds == labels.data)\n","    val_accuracy = (corrects.double() / len(val_dataset)) * 100\n","    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> Val Accuracy: {val_accuracy:.2f}%\")\n","\n","    #Modeli Kaydet\n","    if val_accuracy > best_accuracy:\n","        best_accuracy = val_accuracy\n","        BEST_MODEL_PATH = os.path.join(CHECKPOINT_PATH, \"simple_cnn_best_vize_modeli.pth\")\n","        torch.save(model.state_dict(), BEST_MODEL_PATH)\n","        print(f\" Yeni en iyi model bulundu ve '{BEST_MODEL_PATH}' dosyasına kaydedildi!\")\n","\n","print(\"\\n\\nMODEL EĞİTİMİ TAMAMLANDI\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3kjzrtyfCgd","executionInfo":{"status":"ok","timestamp":1763831947100,"user_tz":-180,"elapsed":1215133,"user":{"displayName":"Murat Habibullaev","userId":"07739432263909480432"}},"outputId":"b30ff137-b13e-4f91-dbe8-25d29eae2743"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Epoch 1/15 -> Val Accuracy: 73.76%\n"," Yeni en iyi model bulundu ve '/content/drive/MyDrive/Deepfake_Projesi/checkpoints/simple_cnn_best_vize_modeli.pth' dosyasına kaydedildi!\n","Epoch 2/15 -> Val Accuracy: 76.49%\n"," Yeni en iyi model bulundu ve '/content/drive/MyDrive/Deepfake_Projesi/checkpoints/simple_cnn_best_vize_modeli.pth' dosyasına kaydedildi!\n","Epoch 3/15 -> Val Accuracy: 81.86%\n"," Yeni en iyi model bulundu ve '/content/drive/MyDrive/Deepfake_Projesi/checkpoints/simple_cnn_best_vize_modeli.pth' dosyasına kaydedildi!\n","Epoch 4/15 -> Val Accuracy: 83.44%\n"," Yeni en iyi model bulundu ve '/content/drive/MyDrive/Deepfake_Projesi/checkpoints/simple_cnn_best_vize_modeli.pth' dosyasına kaydedildi!\n","Epoch 5/15 -> Val Accuracy: 82.58%\n","Epoch 6/15 -> Val Accuracy: 82.51%\n","Epoch 7/15 -> Val Accuracy: 81.86%\n","Epoch 8/15 -> Val Accuracy: 84.73%\n"," Yeni en iyi model bulundu ve '/content/drive/MyDrive/Deepfake_Projesi/checkpoints/simple_cnn_best_vize_modeli.pth' dosyasına kaydedildi!\n","Epoch 9/15 -> Val Accuracy: 81.94%\n","Epoch 10/15 -> Val Accuracy: 80.14%\n","Epoch 11/15 -> Val Accuracy: 81.51%\n","Epoch 12/15 -> Val Accuracy: 83.94%\n","Epoch 13/15 -> Val Accuracy: 82.51%\n","Epoch 14/15 -> Val Accuracy: 82.80%\n","Epoch 15/15 -> Val Accuracy: 82.87%\n","\n","\n","MODEL EĞİTİMİ TAMAMLANDI\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","import shutil\n","\n","# Proje Yolları\n","DRIVE_BASE_PATH = \"/content/drive/MyDrive/Deepfake-Detection-Project\"\n","RAW_DATA_PATH = os.path.join(DRIVE_BASE_PATH, \"datasets\", \"FaceForensics++_C23\")\n","TEST_DATA_PATH = os.path.join(DRIVE_BASE_PATH, \"final_test_data_25_25\") # Anlaşılır olması için yeni bir isim verdim\n","\n","# Önceki test verisi varsa diye temizleyelim\n","if os.path.exists(TEST_DATA_PATH):\n","    shutil.rmtree(TEST_DATA_PATH)\n","    print(f\"Eski '{TEST_DATA_PATH}' klasörü temizlendi.\")\n","\n","# Test klasörlerini yeniden oluşturalım\n","os.makedirs(os.path.join(TEST_DATA_PATH, \"real\"), exist_ok=True)\n","os.makedirs(os.path.join(TEST_DATA_PATH, \"fake\"), exist_ok=True)\n","print(f\"Yeni '{TEST_DATA_PATH}' klasörü oluşturuldu.\")\n","\n","# ==============================================================================\n","# 1. 25 ADET GERÇEK VİDEO AL\n","# ==============================================================================\n","# Eğitimde ilk 250'yi kullanmıştık. Şimdi 250'den sonraki 25 videoyu alıyoruz.\n","real_videos_to_test = sorted(glob.glob(os.path.join(RAW_DATA_PATH, \"original\", \"*.mp4\")))[250:275]\n","\n","for video in real_videos_to_test:\n","    shutil.copy(video, os.path.join(TEST_DATA_PATH, \"real\"))\n","\n","print(f\"-> {len(real_videos_to_test)} adet GERÇEK video 'real' klasörüne başarıyla kopyalandı.\")\n","\n","\n","# ==============================================================================\n","# 2. 25 ADET SAHTE VİDEO AL (SADECE DEEPFAKES KLASÖRÜNDEN)\n","# ==============================================================================\n","# Eğitimde ilk 50'yi kullanmıştık. Şimdi 50'den sonraki 25 videoyu alıyoruz.\n","fake_videos_to_test = sorted(glob.glob(os.path.join(RAW_DATA_PATH, \"Deepfakes\", \"*.mp4\")))[50:75]\n","\n","for video in fake_videos_to_test:\n","    shutil.copy(video, os.path.join(TEST_DATA_PATH, \"fake\"))\n","\n","print(f\"-> {len(fake_videos_to_test)} adet SAHTE video 'fake' klasörüne başarıyla kopyalandı.\")\n","\n","print(\"\\nİstediğin gibi, 25 gerçek ve 25 sahte videodan oluşan test seti hazır!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbtQRI8xDMer","executionInfo":{"status":"ok","timestamp":1763470163224,"user_tz":-180,"elapsed":27630,"user":{"displayName":"Murat Habibullaev","userId":"07739432263909480432"}},"outputId":"856e3914-ff57-43e5-9a94-d666d1097086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Eski '/content/drive/MyDrive/Deepfake_Projesi/final_test_data_25_25' klasörü temizlendi.\n","Yeni '/content/drive/MyDrive/Deepfake_Projesi/final_test_data_25_25' klasörü oluşturuldu.\n","-> 25 adet GERÇEK video 'real' klasörüne başarıyla kopyalandı.\n","-> 25 adet SAHTE video 'fake' klasörüne başarıyla kopyalandı.\n","\n","İstediğin gibi, 25 gerçek ve 25 sahte videodan oluşan test seti hazır!\n"]}]},{"cell_type":"code","source":["# BÖLÜM 9: VİZE İÇİN BASE MODEL FİNAL TESTİ\n","\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from torchvision import transforms\n","from PIL import Image\n","import os, glob, cv2, face_recognition\n","from tqdm.notebook import tqdm\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n","\n","# AYARLAR\n","# Test edilecek modelin '.pth' dosyasının yolu\n","MODEL_PATH = \"/content/drive/MyDrive/Deepfake-Detection-Project/checkpoints/simple_cnn_best_vize_modeli.pth\"\n","\n","# Gerekli tanımlamalar\n","DRIVE_BASE_PATH = \"/content/drive/MyDrive/Deepfake-Detection-Project\"\n","TEST_DATA_PATH = os.path.join(DRIVE_BASE_PATH, \"final_test_data_25_25\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","MODEL_INPUT_SIZE = 224\n","\n","# Test edilecek modelin mimarisi\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1, self.pool1 = nn.Conv2d(3, 16, 3, padding=1), nn.MaxPool2d(2, 2)\n","        self.conv2, self.pool2 = nn.Conv2d(16, 32, 3, padding=1), nn.MaxPool2d(2, 2)\n","        self.conv3, self.pool3 = nn.Conv2d(32, 64, 3, padding=1), nn.MaxPool2d(2, 2)\n","        self.fc1, self.fc2 = nn.Linear(64 * 28 * 28, 512), nn.Linear(512, 1)\n","        self.dropout = nn.Dropout(0.5)\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv1(x))); x = self.pool2(F.relu(self.conv2(x)))\n","        x = self.pool3(F.relu(self.conv3(x))); x = x.view(-1, 64 * 28 * 28)\n","        x = F.relu(self.fc1(x)); x = self.dropout(x); x = self.fc2(x)\n","        return x\n","\n","#Model Yükleme\n","print(\"Basit CNN modeli yükleniyor...\")\n","model = SimpleCNN().to(device)\n","model.load_state_dict(torch.load(MODEL_PATH))\n","model.eval() # Modeli çıkarım moduna al\n","print(\"Model başarıyla yüklendi. Test başlıyor...\")\n","\n","# Video Tahmin Fonksiyonu\n","def predict_video(video_path, model):\n","    transform = transforms.Compose([\n","        transforms.Resize((MODEL_INPUT_SIZE, MODEL_INPUT_SIZE)), transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened(): return \"HATA\"\n","    face_predictions = []\n","    frame_count = 0\n","    while frame_count < 30:\n","        ret, frame = cap.read()\n","        if not ret: break\n","        face_locations = face_recognition.face_locations(frame)\n","        if len(face_locations) > 0:\n","            top, right, bottom, left = face_locations[0]\n","            face_image = frame[top:bottom, left:right]\n","            if face_image.size > 0:\n","                pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n","                input_tensor = transform(pil_image).unsqueeze(0).to(device)\n","                with torch.no_grad():\n","                    output = model(input_tensor)\n","                    prob = torch.sigmoid(output).item()\n","                    face_predictions.append(1 if prob > 0.5 else 0)\n","        frame_count += 1\n","    cap.release()\n","    if not face_predictions: return \"Yüz Bulunamadı\"\n","    final_prediction = 1 if sum(face_predictions) > len(face_predictions) / 2 else 0\n","    return \"FAKE\" if final_prediction == 1 else \"REAL\"\n","\n","#Ana Test Döngüsü\n","all_videos = glob.glob(os.path.join(TEST_DATA_PATH, \"**\", \"*.mp4\"), recursive=True)\n","true_labels, predicted_labels = [], []\n","for video_path in tqdm(all_videos, desc=\"Test Videoları Değerlendiriliyor\"):\n","    ground_truth = \"REAL\" if \"real\" in video_path else \"FAKE\"\n","    prediction = predict_video(video_path, model)\n","    if prediction in [\"REAL\", \"FAKE\"]:\n","        true_labels.append(ground_truth)\n","        predicted_labels.append(prediction)\n","\n","#Sonuçları Hesaplama ve Gösterme\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","precision = precision_score(true_labels, predicted_labels, pos_label='FAKE')\n","recall = recall_score(true_labels, predicted_labels, pos_label='FAKE')\n","f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=[\"REAL\", \"FAKE\"])\n","\n","print(\"\\n\\n\" + \"=\"*30); print(f\"PERFORMANS ANALİZİ SONUÇLARI (SimpleCNN)\"); print(\"=\"*30)\n","print(f\"Toplam Test Edilen Video Sayısı: {len(true_labels)}\")\n","print(f\"Doğruluk (Accuracy): {accuracy * 100:.2f}%\")\n","print(f\"Hassasiyet (Precision for FAKE): {precision * 100:.2f}%\")\n","print(f\"Duyarlılık (Recall for FAKE): {recall * 100:.2f}%\")\n","print(f\"F1 Skoru (F1 Score for FAKE): {f1:.2f}\")\n","print(\"\\nKarışıklık Matrisi (Confusion Matrix):\"); print(\"          Tahmin: REAL | Tahmin: FAKE\")\n","print(f\"Gerçek: REAL | {conf_matrix[0][0]:<11} | {conf_matrix[0][1]}\")\n","print(f\"Gerçek: FAKE | {conf_matrix[1][0]:<11} | {conf_matrix[1][1]}\"); print(\"=\"*30)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377,"referenced_widgets":["62061c97cf824a6f9e7ecdc8bfd9212c","48891aba58b84c3fbdd5e4d5a9252ed3","23a0e9eabad84f9589c16e6f9d84af0f","43779563aed34fc6b00b808e5916172f","d75a76559fa54b10b698c25436d5ca0a","3f77d150f1094b9abe7380eed5a221a0","027bf75bf2024bc48a2c3cca65d8e955","a64b80e07865423b84f57b48e25b0b1b","8da079794ef840afae577002c136862c","86f23544529b4fe8ac527fcaf77a6ca5","7c8e150bf0d54341aee38b3312388f9f"]},"id":"Ro6UdXH88CYG","executionInfo":{"status":"ok","timestamp":1763472121665,"user_tz":-180,"elapsed":617113,"user":{"displayName":"Murat Habibullaev","userId":"07739432263909480432"}},"outputId":"90a9b9a6-e6b0-4979-a13f-78c7509afeee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Basit CNN modeli yükleniyor...\n","Model başarıyla yüklendi. Test başlıyor...\n"]},{"output_type":"display_data","data":{"text/plain":["Test Videoları Değerlendiriliyor:   0%|          | 0/50 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62061c97cf824a6f9e7ecdc8bfd9212c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","==============================\n","PERFORMANS ANALİZİ SONUÇLARI (SimpleCNN)\n","==============================\n","Toplam Test Edilen Video Sayısı: 50\n","Doğruluk (Accuracy): 58.00%\n","Hassasiyet (Precision for FAKE): 83.33%\n","Duyarlılık (Recall for FAKE): 20.00%\n","F1 Skoru (F1 Score for FAKE): 0.32\n","\n","Karışıklık Matrisi (Confusion Matrix):\n","          Tahmin: REAL | Tahmin: FAKE\n","Gerçek: REAL | 24          | 1\n","Gerçek: FAKE | 20          | 5\n","==============================\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"mount_file_id":"12UQMYz6YOSgEn68JqbeXFI4MAXJhUCyq","authorship_tag":"ABX9TyMqXtiE8+6HFCFEzCnjJVEa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"62061c97cf824a6f9e7ecdc8bfd9212c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48891aba58b84c3fbdd5e4d5a9252ed3","IPY_MODEL_23a0e9eabad84f9589c16e6f9d84af0f","IPY_MODEL_43779563aed34fc6b00b808e5916172f"],"layout":"IPY_MODEL_d75a76559fa54b10b698c25436d5ca0a"}},"48891aba58b84c3fbdd5e4d5a9252ed3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f77d150f1094b9abe7380eed5a221a0","placeholder":"​","style":"IPY_MODEL_027bf75bf2024bc48a2c3cca65d8e955","value":"Test Videoları Değerlendiriliyor: 100%"}},"23a0e9eabad84f9589c16e6f9d84af0f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a64b80e07865423b84f57b48e25b0b1b","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8da079794ef840afae577002c136862c","value":50}},"43779563aed34fc6b00b808e5916172f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86f23544529b4fe8ac527fcaf77a6ca5","placeholder":"​","style":"IPY_MODEL_7c8e150bf0d54341aee38b3312388f9f","value":" 50/50 [10:16&lt;00:00, 18.77s/it]"}},"d75a76559fa54b10b698c25436d5ca0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f77d150f1094b9abe7380eed5a221a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"027bf75bf2024bc48a2c3cca65d8e955":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a64b80e07865423b84f57b48e25b0b1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8da079794ef840afae577002c136862c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86f23544529b4fe8ac527fcaf77a6ca5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c8e150bf0d54341aee38b3312388f9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}